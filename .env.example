# ── Fluently Configuration ─────────────────────────────────
# Copy this file to .env and adjust as needed.

# ── LLM / Translation Model ──────────────────────────────
LLM_BASE_URL=http://localhost:11434/v1/    # Ollama (dev) or vLLM endpoint (prod)
LLM_API_KEY=ollama                          # "ollama" for local dev
LLM_MODEL=translategemma:12b

# ── Language Detection ────────────────────────────────────
FASTTEXT_MODEL_PATH=models/fasttext/lid.176.bin
DETECTION_CONFIDENCE_THRESHOLD=0.6827       # 1σ — below this, default to English

# ── Server ────────────────────────────────────────────────
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=info

# ── CORS ──────────────────────────────────────────────────
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# ── Frontend ──────────────────────────────────────────────
VITE_API_URL=/api                           # Proxied in dev, direct URL in prod
